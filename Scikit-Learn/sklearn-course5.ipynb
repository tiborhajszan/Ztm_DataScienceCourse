{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn (Sklearn) Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "0. sklearn workflow overview<br>\n",
    "1. preparing data (exploring, cleaning, transforming, reducing, splitting)<br>\n",
    "2. selecting machine learning model / algorithm<br>\n",
    "3. training algorithm and making predictions<br>\n",
    "4. evaluating algorithm<br>\n",
    "<span style=\"color:orange\">5. improving model</span><br>\n",
    "6. saving and loading algorithm<br>\n",
    "7. putting it all together\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- baseline  \n",
    "first model = baseline model  \n",
    "first prediction = baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- improving model / data perspective  \n",
    "collecting more data (the more data the better)  \n",
    "improving data > adding more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- improving model / algorithm perspective  \n",
    "using a better, more complex algorithm  \n",
    "improving current algorithm with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameters  \n",
    "settings of the algorithm that the user can adjust  \n",
    "basically, hyperparameters are the function parameters of the algorithm instance  \n",
    "hyperparameters are detailed in the documentation of each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameter adjustment methods  \n",
    "by hand (guessing)  \n",
    "random search with randomized search cross validation  \n",
    "exhaustive search (brute force) with grid search cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameters by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading heart disease data into dataframe\n",
    "heart_disease = pandas.read_csv(\"data-heart-disease.csv\")\n",
    "\n",
    "### shuffling heart disease dataframe\n",
    "numpy.random.seed(42)\n",
    "heart_disease = heart_disease.sample(frac=1.0)\n",
    "\n",
    "### splitting data features <> target\n",
    "features = heart_disease.drop(columns=\"target\")\n",
    "target = heart_disease.loc[:, \"target\"]\n",
    "\n",
    "### splitting data train <> test\n",
    "train_index = round(0.7 * heart_disease.index.size)\n",
    "valid_index = round(0.85 * heart_disease.index.size)\n",
    "features_train, target_train = features[:train_index], target[:train_index]\n",
    "features_valid, target_valid = features[train_index:valid_index], target[train_index:valid_index]\n",
    "features_test, target_test = features[valid_index:], target[valid_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classification algorithm evaluation function\n",
    "def evaluatePreds(target_preds, target_test):\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": accuracy_score(y_pred=target_preds, y_true=target_test),\n",
    "        \"Precision\": precision_score(y_pred=target_preds, y_true=target_test),\n",
    "        \"Recall\": recall_score(y_pred=target_preds, y_true=target_test),\n",
    "        \"F1 Score\": f1_score(y_pred=target_preds, y_true=target_test)}\n",
    "    print(f\"\"\"Accuracy: {100.0 * metrics_dict[\"Accuracy\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"Precision: {100.0 * metrics_dict[\"Precision\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"Recall: {100.0 * metrics_dict[\"Recall\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"F1 Score: {100.0 * metrics_dict[\"F1 Score\"]:.3f}%\"\"\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating baseline algorithm\n",
    "\n",
    "### creating, training, predicting algorithm\n",
    "numpy.random.seed(42)\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X=features_train, y=target_train)\n",
    "target_preds = classifier.predict(X=features_valid)\n",
    "\n",
    "### evaluating algorithm\n",
    "algo_baseline = evaluatePreds(target_preds, target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading default hyperparameters of baseline algorithm\n",
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameters to adjust  \n",
    "`max_depth=`  \n",
    "`max_features=`  \n",
    "`min_samples_leaf=`  \n",
    "`min_samples_split=`  \n",
    "`n_estimators=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating adjusted algorithm (max_depth=10)\n",
    "\n",
    "### creating, training, predicting algorithm\n",
    "numpy.random.seed(42)\n",
    "classifier = RandomForestClassifier(max_depth=10)\n",
    "classifier.fit(X=features_train, y=target_train)\n",
    "target_preds = classifier.predict(X=features_valid)\n",
    "\n",
    "### evaluating algorithm\n",
    "algo_hand1 = evaluatePreds(target_preds, target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating adjusted algorithm (n_estimators=500)\n",
    "\n",
    "### creating, training, predicting algorithm\n",
    "numpy.random.seed(42)\n",
    "classifier = RandomForestClassifier(n_estimators=500)\n",
    "classifier.fit(X=features_train, y=target_train)\n",
    "target_preds = classifier.predict(X=features_valid)\n",
    "\n",
    "### evaluating algorithm\n",
    "algo_hand2 = evaluatePreds(target_preds, target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameters with randomized search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading heart disease data into dataframe\n",
    "heart_disease = pandas.read_csv(\"data-heart-disease.csv\")\n",
    "\n",
    "### splitting data features <> target\n",
    "features = heart_disease.drop(columns=\"target\")\n",
    "target = heart_disease.loc[:, \"target\"]\n",
    "\n",
    "### splitting data train <> test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running randomized search\n",
    "\n",
    "### creating search grid\n",
    "search_grid = {\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200]}\n",
    "\n",
    "### creating randomized search algorithm\n",
    "numpy.random.seed(42)\n",
    "rscv_classifier = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(n_jobs=-1),\n",
    "    param_distributions=search_grid,\n",
    "    n_iter=10, cv=5, verbose=2)\n",
    "\n",
    "### training randomized search algorithm\n",
    "rscv_classifier.fit(X=features, y=target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading best parameters\n",
    "rscv_classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm\n",
    "target_preds = rscv_classifier.predict(features_test)\n",
    "algo_rscv = evaluatePreds(target_preds, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading california housing dataset\n",
    "housing_dict = fetch_california_housing()\n",
    "\n",
    "### creating california housing dataframe\n",
    "housing_df = pandas.DataFrame(data=housing_dict[\"data\"], columns=housing_dict[\"feature_names\"])\n",
    "housing_df[\"MedHouseVal\"] = housing_dict[\"target\"]\n",
    "\n",
    "### splitting data features/target\n",
    "features = housing_df.drop(columns=\"MedHouseVal\")\n",
    "target = housing_df.loc[:, \"MedHouseVal\"]\n",
    "\n",
    "### splitting data train/test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)\n",
    "target_test: numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### random forest regressor training and prediction\n",
    "\n",
    "### instantiating model\n",
    "numpy.random.seed(42)\n",
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "### training model / prediction\n",
    "regressor.fit(X=features_train, y=target_train)\n",
    "target_preds = regressor.predict(X=features_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Ztm_Code': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d135630f3aba14e844087208813e69ab65195afd4ab5238b4ebe56330592421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
