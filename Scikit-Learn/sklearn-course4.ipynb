{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn (sklearn) Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "0. sklearn workflow overview<br>\n",
    "1. preparing data (collecting, exploring, cleaning, transforming, reducing, splitting)<br>\n",
    "2. defining problem / selecting machine learning model<br>\n",
    "3. training model and making predictions<br>\n",
    "<span style=\"color:orange\">4. evaluating model</span><br>\n",
    "5. improving model<br>\n",
    "6. saving and loading model<br>\n",
    "7. putting it all together\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- resources  \n",
    "[sklearn documentation > model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n",
    "[statquest youtube video: ROC and AUC explained](https://www.youtube.com/watch?v=4jRBRDbJemM)  \n",
    "[sklearn documentation > ROC curve for multiclass classification models](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)  \n",
    "<span style=\"color:red\">>>> One-vs-Rest multiclass ROC<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- sklearn built-in evaluation methods  \n",
    "`.score()` method  \n",
    "cross valiadion  \n",
    "metric functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- cross validation  \n",
    "creates `cv=k` different train/test splits from the same dataset (k-fold cross validation)  \n",
    "trains and scores the algorithm on all splits > training covers the entire dataset  \n",
    "scoring metric is defined by the `scoring=` parameter  \n",
    "`scoring=None` invokes the default scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- area under the ROC curve (AUC)  \n",
    "integral of ROC curve > ranges between 0.0-1.0  \n",
    "used to compare the performance of different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- confusion matrix  \n",
    "two-dimensional array of targets (rows) vs predictions (columns)  \n",
    "a quick way to compare targets to predictions  \n",
    "gives an idea of where the model is confused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- classification report  \n",
    "a summary table of several classification metrics  \n",
    "**precision:** true within-class predictions / all within-class predictions  \n",
    "**recall:** true within-class predictions / all within-class targets  \n",
    "**f1-score:** within-class mean of precision and recall  \n",
    "**support:** all within-class targets  \n",
    "**accuracy:** true predictions / all predictions  \n",
    "**macro average:** accross-class means of precision, recall, and f1-score  \n",
    "**weighted average:** support-weighted accross-class means of precision, recall, and f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- regression model metrics  \n",
    "coefficient of determination (R^2)  \n",
    "mean absolute error (MAE)  \n",
    "mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- coding tricks within jupyter notebook  \n",
    "**`!command`, e.g., `!dir`** runs terminal command within jupyter notebook  \n",
    "**`sklearn.__version__`** displays version of installed module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading heart disease classification data into dataframe\n",
    "heart_disease = pandas.read_csv(filepath_or_buffer=\"data-heart-disease.csv\")\n",
    "\n",
    "### splitting data features/target\n",
    "features = heart_disease.drop(columns=\"target\")\n",
    "target = heart_disease.loc[:, \"target\"]\n",
    "\n",
    "### splitting data train/test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating random forest classifier\n",
    "\n",
    "### instantiating model\n",
    "numpy.random.seed(42)\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "### training model\n",
    "classifier.fit(X=features_train, y=target_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- classification model metrics  \n",
    "**accuracy:** true predictions / all predictions  \n",
    "**confusion matrix:** see below  \n",
    "**classification report:** see below  \n",
    "**receiver operating characteristic (ROC) curve** see below  \n",
    "**area under the ROC curve (AUC)** see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- `.score()` method  \n",
    "default `.score()` metric for classification models is accuracy (true predictions / all predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with model.score() method on training data\n",
    "classifier.score(X=features_train, y=target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with model.score() method on test data\n",
    "classifier.score(X=features_test, y=target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with default score (accuracy) cross validation ----------------------------------------------------\n",
    "cv_list = cross_val_score(estimator=classifier, X=features, y=target, cv=5, scoring=None)\n",
    "cv_mean = numpy.mean(cv_list)\n",
    "cv_list, cv_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- receiver operating characteristic (ROC) curve  \n",
    "plots true positive rate (tpr) over false positive rate (fpr)  \n",
    "visualizes algorithm performance at various algorithm decision thresholds  \n",
    "suitable for binary classification models  \n",
    "**true negative rate = specificity:** true negative predictions / all negative targets  \n",
    "**false positive rate = 1 - specificity:** false positive predictions / all negative targets  \n",
    "**false negative rate = 1 - sensitivity:** false negative predictions / all positive targets  \n",
    "**true positive rate = sensitivity = recall:** true positive predictions / all positive targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- ROC curve for multiclass classification models  \n",
    "a ROC curve works with binary output, so multiclass output must be binarized  \n",
    "one-vs-rest binarization: comparing each class to all the others  \n",
    "one-vs-one binarization: comparing every pairwise combination of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for plotting ROC curve\n",
    "\n",
    "### function init\n",
    "def plotRoc(plot_title, tpr, fpr):\n",
    "    \"\"\"\n",
    "    Plots ROC curve, i.e., true positive rate (tpr) over false positive rate (fpr)\n",
    "    \"\"\"\n",
    "\n",
    "    ### plotting ROC curve\n",
    "    pyplot.plot(fpr, tpr, color=\"orange\", label=\"ROC Curve\")\n",
    "\n",
    "    ### plotting baseline\n",
    "    pyplot.plot([0,1], [0,1], color=\"blue\", linestyle=\"--\", label=\"Guessing\")\n",
    "\n",
    "    ### customizing plot\n",
    "    pyplot.title(plot_title)\n",
    "    pyplot.ylabel(\"True Positive Rate\")\n",
    "    pyplot.xlabel(\"False Positive Rate\")\n",
    "    pyplot.legend()\n",
    "\n",
    "    ### rendering plot\n",
    "    pyplot.show()\n",
    "\n",
    "    ### function termination\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with ROC curve\n",
    "target_probs_positive = classifier.predict_proba(X=features_test)[:, 1]\n",
    "classifier_fpr, classifier_tpr, classifier_thresholds = roc_curve(y_true=target_test, y_score=target_probs_positive)\n",
    "plotRoc(\"Receiver Operating Characteristic (ROC)\", classifier_tpr, classifier_fpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### perfect ROC curve\n",
    "perfect_fpr, perfect_tpr, perfect_threshold = roc_curve(y_true=target_test, y_score=target_test)\n",
    "plotRoc(\"Perfect ROC\", perfect_tpr, perfect_fpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with AUC score ------------------------------------------------------------------------------------\n",
    "model_auc = roc_auc_score(y_true=target_test, y_score=target_probs_positive)\n",
    "perfect_auc = roc_auc_score(y_true=target_test, y_score=target_test)\n",
    "model_auc, perfect_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with confusion matrix -----------------------------------------------------------------------------\n",
    "target_preds = classifier.predict(X=features_test)\n",
    "pandas.crosstab(index=target_test, rownames=[\"Targets\"], columns=target_preds, colnames=[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualizing confusion matrix with sklearn --------------------------------------------------------------------------\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=target_test, y_pred=target_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with classification report ------------------------------------------------------------------------\n",
    "print(classification_report(y_true=target_test, y_pred=target_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading california housing dataset\n",
    "housing_dict = fetch_california_housing()\n",
    "\n",
    "### creating california housing dataframe\n",
    "housing_df = pandas.DataFrame(data=housing_dict[\"data\"], columns=housing_dict[\"feature_names\"])\n",
    "housing_df[\"MedHouseVal\"] = housing_dict[\"target\"]\n",
    "\n",
    "### splitting data features/target\n",
    "features = housing_df.drop(columns=\"MedHouseVal\")\n",
    "target = housing_df.loc[:, \"MedHouseVal\"]\n",
    "\n",
    "### splitting data train/test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)\n",
    "target_test: numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating random forest regressor\n",
    "\n",
    "### instantiating model\n",
    "numpy.random.seed(42)\n",
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "### training model\n",
    "regressor.fit(X=features_train, y=target_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- `.score()` method  \n",
    "default `.score()` metric for regression models is [coefficient of determination (r^2)](https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with .score() method on test data\n",
    "regressor.score(X=features_test, y=target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- coefficient of determination (r2-score)  \n",
    "when a model predicts the mean of targets, its r2-score is 0.0  \n",
    "when a model perfectly predicts all targets, its r2-score is 1.0  \n",
    "`numpy.full(shape=, fill_value=)` creates an array of `shape=` filled with `fill_value=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting r2-score of 0.0\n",
    "target_test_mean = numpy.full(shape=len(target_test), fill_value=target_test.mean())\n",
    "r2_score(y_true=target_test, y_pred=target_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting r2-score of 1.0\n",
    "r2_score(y_true=target_test, y_pred=target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- mean absolute error (MAE)  \n",
    "mean of absolute differences between predictions and targets  \n",
    "represents the linear magnitude of prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean absolute error with sklearn function\n",
    "target_preds = regressor.predict(X=features_test)\n",
    "mean_absolute_error(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean absolute error step-by-step\n",
    "error_df = pandas.DataFrame(data={\"target preds\": target_preds, \"target test\": target_test})\n",
    "error_df[\"differences\"] = numpy.abs(error_df[\"target preds\"] - error_df[\"target test\"])\n",
    "error_df[\"differences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- mean squared error (MSE)  \n",
    "mean of squared differences between predictions and targets  \n",
    "squaring emphasizes large errors and diminishes small errors  \n",
    "there is also root mean squared error (RMSE) - see sklearn documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean squared error with sklearn function\n",
    "mean_squared_error(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean squared error step-by-step\n",
    "error_df[\"squared differences\"] = numpy.square(error_df[\"differences\"])\n",
    "error_df[\"squared differences\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Ztm_Code': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d135630f3aba14e844087208813e69ab65195afd4ab5238b4ebe56330592421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
