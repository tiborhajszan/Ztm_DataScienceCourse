{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn (Sklearn) Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "0. sklearn workflow overview<br>\n",
    "1. preparing data (exploring, cleaning, transforming, reducing, splitting)<br>\n",
    "2. selecting machine learning model / algorithm<br>\n",
    "3. algorithm training and prediction<br>\n",
    "<span style=\"color:orange\">4. algorithm evaluation</span><br>\n",
    "5. improving algorithm<br>\n",
    "6. saving and loading algorithm<br>\n",
    "7. putting it all together\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- resources  \n",
    "[sklearn documentation > model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n",
    "[statquest youtube video: ROC and AUC explained](https://www.youtube.com/watch?v=4jRBRDbJemM)  \n",
    "[sklearn documentation > ROC curve for multiclass classification algorithms](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)  \n",
    "<span style=\"color:red\">>>> One-vs-Rest multiclass ROC<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- sklearn built-in evaluation techniques  \n",
    "`.score()` method  \n",
    "cross valiadion  \n",
    "metric functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- cross validation  \n",
    "creates `cv=k` different train/test splits from the same dataset (k-fold cross validation)  \n",
    "trains and scores the algorithm on all splits > training covers the entire dataset  \n",
    "scoring metric is defined by the `scoring=` parameter > `scoring=None` invokes the default scorer  \n",
    "provides algorithm metric as mean of scores from all splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- regression model metrics  \n",
    "coefficient of determination (R^2)  \n",
    "mean absolute error (MAE)  \n",
    "mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- coding tricks within jupyter notebook  \n",
    "**`!command`, e.g., `!dir`** runs terminal command within jupyter notebook  \n",
    "**`sklearn.__version__`** displays version of installed module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading heart disease classification data into dataframe\n",
    "heart_disease = pandas.read_csv(filepath_or_buffer=\"data-heart-disease.csv\")\n",
    "\n",
    "### splitting data features/target\n",
    "features = heart_disease.drop(columns=\"target\")\n",
    "target = heart_disease.loc[:, \"target\"]\n",
    "\n",
    "### splitting data train/test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### random forest classifier training and prediction\n",
    "\n",
    "### instantiating model\n",
    "numpy.random.seed(42)\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "### training model / prediction\n",
    "classifier.fit(X=features_train, y=target_train)\n",
    "target_preds = classifier.predict(X=features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- classification model metrics  \n",
    "`.score()` method, confusion matrix, classification report, ROC curve, AUC  \n",
    "accuracy, precision, recall, f1-score, TNR, FPR, FNR, TPR  \n",
    "cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- `.score()` method  \n",
    "default `.score()` and cross validation metric for classification algorithms is accuracy  \n",
    "**accuracy:** true predictions / all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with .score() method on training data\n",
    "classifier.score(X=features_train, y=target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with .score() method on test data\n",
    "classifier.score(X=features_test, y=target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- accuracy  \n",
    "formula = true predictions / all predictions  \n",
    "accuracy is a good metric when classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with accuracy score\n",
    "accuracy_score(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated default score (accuracy)\n",
    "numpy.random.seed(42)\n",
    "cv_accuracy = cross_val_score(estimator=classifier, X=features, y=target, cv=5, scoring=None)\n",
    "cv_accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- confusion matrix  \n",
    "two-dimensional array of targets (rows) vs predictions (columns)  \n",
    "a quick way to compare targets to predictions  \n",
    "gives an idea of where the algorithm is confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with confusion matrix\n",
    "pandas.crosstab(index=target_test, rownames=[\"Targets\"], columns=target_preds, colnames=[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualizing confusion matrix with sklearn\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=target_test, y_pred=target_preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- classification report  \n",
    "a summary table of several classification metrics  \n",
    "**precision:** true within-class predictions / all within-class predictions  \n",
    "**recall:** true within-class predictions / all within-class targets  \n",
    "**f1-score:** within-class mean of precision and recall  \n",
    "**support:** all within-class targets  \n",
    "**accuracy:** true predictions / all predictions  \n",
    "**macro average:** accross-class means of precision, recall, and f1-score  \n",
    "**weighted average:** support-weighted accross-class means of precision, recall, and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with classification report\n",
    "print(classification_report(y_true=target_test, y_pred=target_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- precision  \n",
    "formula = true positive predictions / all positive predictions  \n",
    "precision is a better metric when classes are unbalanced  \n",
    "precision becomes important when false positives are concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with precision score\n",
    "precision_score(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated precision\n",
    "numpy.random.seed(42)\n",
    "cv_precision = cross_val_score(estimator=classifier, X=features, y=target, cv=5, scoring=\"precision\")\n",
    "cv_precision.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- recall  \n",
    "formula = true positive predictions / all positive targets  \n",
    "recall is a better metric when classes are unbalanced  \n",
    "recall becomes important when false negatives are concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with recall score\n",
    "recall_score(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated recall\n",
    "numpy.random.seed(42)\n",
    "cv_recall = cross_val_score(estimator=classifier, X=features, y=target, cv=5, scoring=\"recall\")\n",
    "cv_recall.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- f1-score  \n",
    "formula = mean of precision and recall  \n",
    "f1-score is a better metric when classes are unbalanced  \n",
    "f1-score becomes important when both false positives and false negatives are concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with f1-score\n",
    "f1_score(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated f1-score\n",
    "numpy.random.seed(42)\n",
    "cv_f1 = cross_val_score(estimator=classifier, X=features, y=target, cv=5, scoring=\"f1\")\n",
    "cv_f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- receiver operating characteristic (ROC) curve  \n",
    "plots true positive rate (tpr) over false positive rate (fpr)  \n",
    "visualizes algorithm performance at various algorithm decision thresholds  \n",
    "suitable for binary classification models  \n",
    "**true negative rate = specificity:** true negative predictions / all negative targets  \n",
    "**false positive rate = 1 - specificity:** false positive predictions / all negative targets  \n",
    "**false negative rate = 1 - sensitivity:** false negative predictions / all positive targets  \n",
    "**true positive rate = sensitivity = recall:** true positive predictions / all positive targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- ROC curve for multiclass classification models  \n",
    "a ROC curve works with binary output, so multiclass output must be binarized  \n",
    "one-vs-rest binarization: comparing each class to all the others  \n",
    "one-vs-one binarization: comparing every pairwise combination of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for plotting ROC curve\n",
    "\n",
    "### function init\n",
    "def plotRoc(plot_title, tpr, fpr):\n",
    "    \"\"\"\n",
    "    Plots ROC curve, i.e., true positive rate (tpr) over false positive rate (fpr)\n",
    "    \"\"\"\n",
    "\n",
    "    ### plotting ROC curve\n",
    "    pyplot.plot(fpr, tpr, color=\"orange\", label=\"ROC Curve\")\n",
    "\n",
    "    ### plotting baseline\n",
    "    pyplot.plot([0,1], [0,1], color=\"blue\", linestyle=\"--\", label=\"Guessing\")\n",
    "\n",
    "    ### customizing plot\n",
    "    pyplot.title(plot_title)\n",
    "    pyplot.ylabel(\"True Positive Rate\")\n",
    "    pyplot.xlabel(\"False Positive Rate\")\n",
    "    pyplot.legend()\n",
    "\n",
    "    ### rendering plot\n",
    "    pyplot.show()\n",
    "\n",
    "    ### function termination\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with ROC curve\n",
    "target_probs_positive = classifier.predict_proba(X=features_test)[:, 1]\n",
    "classifier_fpr, classifier_tpr, classifier_thresholds = roc_curve(y_true=target_test, y_score=target_probs_positive)\n",
    "plotRoc(\"Receiver Operating Characteristic (ROC)\", classifier_tpr, classifier_fpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting ROC curve with sklearn\n",
    "RocCurveDisplay.from_predictions(y_pred=target_probs_positive, y_true=target_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### perfect ROC curve\n",
    "perfect_fpr, perfect_tpr, perfect_threshold = roc_curve(y_true=target_test, y_score=target_test)\n",
    "plotRoc(\"Perfect ROC\", perfect_tpr, perfect_fpr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- area under the ROC curve (AUC)  \n",
    "integral of ROC curve > ranges between 0.0-1.0  \n",
    "used to compare the performance of different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating model with AUC score ------------------------------------------------------------------------------------\n",
    "classifier_auc = roc_auc_score(y_true=target_test, y_score=target_probs_positive)\n",
    "perfect_auc = roc_auc_score(y_true=target_test, y_score=target_test)\n",
    "classifier_auc, perfect_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading california housing dataset\n",
    "housing_dict = fetch_california_housing()\n",
    "\n",
    "### creating california housing dataframe\n",
    "housing_df = pandas.DataFrame(data=housing_dict[\"data\"], columns=housing_dict[\"feature_names\"])\n",
    "housing_df[\"MedHouseVal\"] = housing_dict[\"target\"]\n",
    "\n",
    "### splitting data features/target\n",
    "features = housing_df.drop(columns=\"MedHouseVal\")\n",
    "target = housing_df.loc[:, \"MedHouseVal\"]\n",
    "\n",
    "### splitting data train/test\n",
    "numpy.random.seed(42)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)\n",
    "target_test: numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### random forest regressor training and prediction\n",
    "\n",
    "### instantiating model\n",
    "numpy.random.seed(42)\n",
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "### training model / prediction\n",
    "regressor.fit(X=features_train, y=target_train)\n",
    "target_preds = regressor.predict(X=features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- `.score()` method  \n",
    "default `.score()` and cross validation metric for regression models is [coefficient of determination (r^2)](https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with .score() method on test data\n",
    "regressor.score(X=features_test, y=target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- coefficient of determination (r2-score)  \n",
    "when a model predicts the mean of targets, its r2-score is 0.0  \n",
    "when a model perfectly predicts all targets, its r2-score is 1.0  \n",
    "`numpy.full(shape=, fill_value=)` creates an array of `shape=` filled with `fill_value=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting r2-score of 0.0\n",
    "target_test_mean = numpy.full(shape=len(target_test), fill_value=target_test.mean())\n",
    "r2_score(y_true=target_test, y_pred=target_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting r2-score of 1.0\n",
    "r2_score(y_true=target_test, y_pred=target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated default score (r2-score)\n",
    "numpy.random.seed(42)\n",
    "cv_r2 = cross_val_score(estimator=regressor, X=features, y=target, cv=3, scoring=None)\n",
    "cv_r2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- mean absolute error (MAE)  \n",
    "mean of absolute differences between predictions and targets  \n",
    "represents the linear magnitude of prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean absolute error with sklearn function\n",
    "mean_absolute_error(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean absolute error step-by-step\n",
    "error_df = pandas.DataFrame(data={\"target preds\": target_preds, \"target test\": target_test})\n",
    "error_df[\"differences\"] = numpy.abs(error_df[\"target preds\"] - error_df[\"target test\"])\n",
    "error_df[\"differences\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated mean absolute error\n",
    "numpy.random.seed(42)\n",
    "cv_mae = cross_val_score(estimator=regressor, X=features, y=target, cv=3, scoring=\"neg_mean_absolute_error\")\n",
    "cv_mae.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- mean squared error (MSE)  \n",
    "mean of squared differences between predictions and targets  \n",
    "squaring emphasizes large errors and diminishes small errors  \n",
    "there is also root mean squared error (RMSE) - see sklearn documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean squared error with sklearn function\n",
    "mean_squared_error(y_true=target_test, y_pred=target_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing mean squared error step-by-step\n",
    "error_df[\"squared differences\"] = numpy.square(error_df[\"differences\"])\n",
    "error_df[\"squared differences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- cross validation  \n",
    "cross validation scoring is also available for evaluating regression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating algorithm with cross-validated mean squared error\n",
    "numpy.random.seed(42)\n",
    "cv_mse = cross_val_score(estimator=regressor, X=features, y=target, cv=3, scoring=\"neg_mean_squared_error\")\n",
    "cv_mse.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Ztm_Code': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d135630f3aba14e844087208813e69ab65195afd4ab5238b4ebe56330592421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
