{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn (Sklearn) Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "0. sklearn workflow overview<br>\n",
    "1. preparing data (exploring, cleaning, transforming, reducing, splitting)<br>\n",
    "2. selecting machine learning model / algorithm<br>\n",
    "3. training algorithm and making predictions<br>\n",
    "4. evaluating algorithm<br>\n",
    "<span style=\"color:orange\">5. improving the model</span><br>\n",
    "6. saving and loading algorithm<br>\n",
    "7. putting it all together\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- resources  \n",
    "https://colab.research.google.com/drive/1ISey96a5Ag6z2CvVZKVqTKNWRwZbZl0m  \n",
    "colab notebook correcting a model comparison error in the lesson videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- baseline  \n",
    "first model = baseline model  \n",
    "first prediction = baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- improving the model / data perspective  \n",
    "collecting more data (the more data the better)  \n",
    "improving current data by adding more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- correlation analysis  \n",
    "when two features are highly correlated, one of them may be removed from the model  \n",
    "backward feature selection: checking whether removing features reduces model perfomance  \n",
    "forward feature selection: checking whether adding features improves model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- improving the model / algorithm perspective  \n",
    "using a better, more complex algorithm  \n",
    "improving current algorithm with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameters  \n",
    "settings of the algorithm that the user can adjust  \n",
    "hyperparameters are basically function parameters of the algorithm instance  \n",
    "hyperparameters are detailed in the documentation of each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameter adjustment methods  \n",
    "by hand (user guessing)  \n",
    "randomized search cross validation (machine guessing)  \n",
    "grid search cross validation (brute force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy, pandas\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data\n",
    "\n",
    "### loading heart disease data into dataframe\n",
    "heart_disease = pandas.read_csv(\"data-heart-disease.csv\")\n",
    "\n",
    "### splitting data features <> target\n",
    "features = heart_disease.drop(columns=\"target\")\n",
    "target = heart_disease.loc[:, \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classification algorithm evaluation function\n",
    "def evaluateAlgo(algorithm, features, target):\n",
    "    numpy.random.seed(42)\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": cross_val_score(estimator=algorithm, X=features, y=target, cv=5, scoring=\"accuracy\").mean(),\n",
    "        \"Precision\": cross_val_score(estimator=algorithm, X=features, y=target, cv=5, scoring=\"precision\").mean(),\n",
    "        \"Recall\": cross_val_score(estimator=algorithm, X=features, y=target, cv=5, scoring=\"recall\").mean(),\n",
    "        \"F1 Score\": cross_val_score(estimator=algorithm, X=features, y=target, cv=5, scoring=\"f1\").mean()}\n",
    "    print(f\"\"\"Accuracy: {100.0 * metrics_dict[\"Accuracy\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"Precision: {100.0 * metrics_dict[\"Precision\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"Recall: {100.0 * metrics_dict[\"Recall\"]:.3f}%\"\"\")\n",
    "    print(f\"\"\"F1 Score: {100.0 * metrics_dict[\"F1 Score\"]:.3f}%\"\"\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameters by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating baseline algorithm\n",
    "algo_baseline = evaluateAlgo(RandomForestClassifier(n_jobs=-1), features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading default hyperparameters of baseline algorithm\n",
    "RandomForestClassifier(n_jobs=-1).get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- hyperparameters to adjust  \n",
    "`max_depth=`  \n",
    "`max_features=`  \n",
    "`min_samples_leaf=`  \n",
    "`min_samples_split=`  \n",
    "`n_estimators=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating adjusted algorithm (max_depth=10)\n",
    "algo_hand1 = evaluateAlgo(RandomForestClassifier(max_depth=10, n_jobs=-1), features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and evaluating adjusted algorithm (n_estimators=500)\n",
    "algo_hand2 = evaluateAlgo(RandomForestClassifier(n_estimators=500, n_jobs=-1), features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameters with randomized search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running randomized search\n",
    "\n",
    "### creating random grid\n",
    "random_grid = {\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200]}\n",
    "\n",
    "### creating randomized search object\n",
    "classifier_rscv = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(n_jobs=-1),\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=10, cv=5, verbose=True)\n",
    "\n",
    "### training randomized search object\n",
    "numpy.random.seed(42)\n",
    "classifier_rscv.fit(X=features, y=target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading best parameters\n",
    "classifier_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating randomized search object\n",
    "algo_rscv = evaluateAlgo(classifier_rscv.best_estimator_, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameters with grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running grid search\n",
    "\n",
    "### creating search grid\n",
    "search_grid = {\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200]}\n",
    "\n",
    "### creating grid search object\n",
    "classifier_gscv = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(n_jobs=-1),\n",
    "    param_grid=search_grid,\n",
    "    cv=5, verbose=True)\n",
    "\n",
    "### training grid search object\n",
    "numpy.random.seed(42)\n",
    "classifier_gscv.fit(X=features, y=target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading best parameters\n",
    "classifier_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating grid search object\n",
    "algo_gscv = evaluateAlgo(classifier_gscv.best_estimator_, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating metrics dataframe\n",
    "metrics_df = pandas.DataFrame(data={\n",
    "    \"Baseline\": algo_baseline,\n",
    "    \"max_depth=10\": algo_hand1,\n",
    "    \"n_estimators=500\": algo_hand2,\n",
    "    \"Random Search\": algo_rscv,\n",
    "    \"Grid Search\": algo_gscv})\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting metrics dataframe\n",
    "metrics_df.plot.bar(figsize=(10,8));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Ztm_Code': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d135630f3aba14e844087208813e69ab65195afd4ab5238b4ebe56330592421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
