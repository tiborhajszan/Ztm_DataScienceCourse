{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Sale Price of Bulldozers (Kaggle Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict the sale price of bulldozers at auction.  \n",
    "Predictions are based on usage, equipment type, and configuaration.  \n",
    "The data is sourced from auction result postings.  \n",
    "Type of machine learning problem: **supervised learning / time series regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition evaluation metric was *root mean squared log error (RMSLE)*.  \n",
    "**Project goal:** To minimize the difference between actual and predicted prices, i.e., to minimize RMSLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is downloaded from the *Bluebook for Bulldozers* past Kaggle competition:  \n",
    "[Bluebook for Bulldozers Kaggle Competition](https://www.kaggle.com/c/bluebook-for-bulldozers/overview)  \n",
    "There are three main datasets:  \n",
    "* **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012.  \n",
    "The score on this set was used to create the public leaderboard.\n",
    "* **Test.csv** is the test set, which contains data from May 1, 2012 - November 30, 2012.  \n",
    "The score on this set determined the final rank for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle provided a data dictiorany for these datasets.  \n",
    "See `data-dictionary.xlsx` in the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing standard libraries\n",
    "from typing import List, Dict\n",
    "\n",
    "### importing data analysis libraries\n",
    "import numpy, pandas\n",
    "from pandas import read_csv, DataFrame, Series\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "### importing machine learning libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing dates**\n",
    "\n",
    "Working with time series data requires date/time information to be in python datetime format for easy processing.  \n",
    "Date/time columns are parsed into datetime format using the `parse_dates=` parameter of `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing training and validation datasets from file\n",
    "import_df: DataFrame = read_csv(\n",
    "    filepath_or_buffer=\"../Large-Files/data-train-valid.csv\",\n",
    "    parse_dates=[\"saledate\"],\n",
    "    low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the distribution of target variable\n",
    "import_df[\"SalePrice\"].plot.hist(color=\"steelblue\")\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Distribution of Bulldozer Sale Prices\")\n",
    "pyplot.ylabel(ylabel=\"Sale Count\")\n",
    "pyplot.xlabel(xlabel=\"Sale Price ($)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: Sale date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the correlation between sale price and sale date\n",
    "import_df[:500].plot.scatter(y=\"SalePrice\", x=\"saledate\", c=\"steelblue\", s=50)\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Sale Price and Sale Date\")\n",
    "pyplot.ylabel(ylabel=\"Sale Price ($)\")\n",
    "pyplot.xlabel(xlabel=\"Sale Date\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: State of sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the distribution of sales by state\n",
    "import_df[\"state\"].value_counts().sort_index(ascending=True).plot.bar(figsize=(12,5))\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Distribution of Sales by States\")\n",
    "pyplot.ylabel(ylabel=\"Sale Counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-driven data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "When there are a huge amount of features, it may be better to start building a machine learning model right away.  \n",
    "Model-driven data exploration lets the machine learning algorithm select the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "Feature engineering means processing data in the dataset.  \n",
    "It includes transforming existing data and/or creating new data from existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting dataframe by date**\n",
    "\n",
    "When working with time series, it is better to sort data by date/time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: sorting dataframe by sale date\n",
    "def sortDataFrame(df:DataFrame) -> None:\n",
    "    df.sort_values(by=\"saledate\", ascending=True, ignore_index=True, inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enriching dataset with date information**\n",
    "\n",
    "Using the `pandas.dt` interface, additional data is extracted from datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: extracting date information from sale date column\n",
    "def appendDateInfo(df:DataFrame) -> None:\n",
    "    df[\"saleYear\"] = df[\"saledate\"].dt.year\n",
    "    df[\"saleMonth\"] = df[\"saledate\"].dt.month\n",
    "    df[\"saleDay\"] = df[\"saledate\"].dt.day\n",
    "    df[\"saleDayOfWeek\"] = df[\"saledate\"].dt.day_of_week\n",
    "    df[\"saleDayOfYear\"] = df[\"saledate\"].dt.day_of_year\n",
    "    df.drop(columns=\"saledate\", inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions: Preprocessing numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manipulating datatypes: Pandas api.types interface**\n",
    "\n",
    "Datatypes can be manipulated with the `pandas.api.types` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical concept**\n",
    "\n",
    "The mean is much more sensitive to outliers than the median.  \n",
    "Using the median is advised with large datasets full of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important machine learning concept**\n",
    "\n",
    "Filling missing numerical values with any statistical endpoints must be done after splitting the dataset!  \n",
    "Validation / test data in any form must not be used in training the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: filling missing numerical values with median\n",
    "def cleanNumerics(df:DataFrame) -> None:\n",
    "    for column,values in df.items(): # iterating through dataframe\n",
    "        if pandas.api.types.is_numeric_dtype(values): # selecting numeric columns\n",
    "            df[column+\"_missing\"] = values.isna() # saving location of missing values\n",
    "            df[column] = values.fillna(value=values.median()) # filling with median\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions: Preprocessing string data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas category datatype**\n",
    "\n",
    "One way to convert strings into numbers is to use the pandas category datatype.  \n",
    "The category datatype represents missing values with -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas .cat interface**\n",
    "\n",
    "The category datatype is manipulated with the `pandas.cat` interface.  \n",
    "`dataframe[column].cat.categories` = lists all categories in the given dataframe column.  \n",
    "`dataframe[column].cat.codes` = lists all integer codes in the given dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: converting str > category > int and filling missing values\n",
    "def cleanStrings(df:DataFrame) -> None:\n",
    "    for column,values in df.items(): # iterating through dataframe\n",
    "        if pandas.api.types.is_string_dtype(values): # selecting string columns\n",
    "            df[column] = values.astype('category').cat.as_ordered() # converting str > category\n",
    "            df[column] = df[column].cat.codes + 1 # converting category > int (missing values = 0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data: Engineering, splitting, and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### date/time feature engineering\n",
    "sortDataFrame(df=import_df)\n",
    "appendDateInfo(df=import_df)\n",
    "\n",
    "### splitting dataset training / validation\n",
    "training_df: DataFrame = import_df.loc[import_df[\"saleYear\"] != 2012].copy(deep=True)\n",
    "validation_df: DataFrame = import_df.loc[import_df[\"saleYear\"] == 2012].copy(deep=True)\n",
    "\n",
    "### preprocessing numeric columns\n",
    "cleanNumerics(df=training_df)\n",
    "cleanNumerics(df=validation_df)\n",
    "\n",
    "### preprocessing string / categorical columns\n",
    "cleanStrings(df=training_df)\n",
    "cleanStrings(df=validation_df)\n",
    "\n",
    "### splitting datasets features / targets\n",
    "train_features: DataFrame = training_df.drop(columns=\"SalePrice\")\n",
    "train_targets: Series = training_df.loc[:,\"SalePrice\"].copy(deep=True)\n",
    "valid_features: DataFrame = validation_df.drop(columns=\"SalePrice\")\n",
    "valid_targets: Series = validation_df.loc[:,\"SalePrice\"].copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating initial random forest regressor\n",
    "regressor = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "### training algorithm\n",
    "regressor.fit(X=train_features, y=train_targets);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating scorer object: root mean squared log error\n",
    "rmsle_scorer = make_scorer(mean_squared_log_error, greater_is_better=False, squared=False)\n",
    "\n",
    "### scoring initial model using rmsle scorer\n",
    "rmsle_train: float = rmsle_scorer(estimator=regressor, X=train_features, y_true=train_targets)\n",
    "rmsle_valid: float = rmsle_scorer(estimator=regressor, X=valid_features, y_true=valid_targets)\n",
    "rmsle_train, rmsle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing data**\n",
    "\n",
    "It takes a lot of time to train an algorithm on a large training dataset.  \n",
    "The solution is to use only a fraction of the training dataset for experimenting.  \n",
    "The final model is trained on the entire training dataset using the best hyperparameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_samples argument**\n",
    "\n",
    "With random forest regressor, the fraction of training dataset is specified by using the `max_samples` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating reduced data random forest regressor\n",
    "regressor_reduced = RandomForestRegressor(random_state=42, max_samples=10000, n_jobs=-1)\n",
    "\n",
    "### training algorithm\n",
    "regressor_reduced.fit(X=train_features, y=train_targets)\n",
    "\n",
    "### scoring reduced data model using rmsle scorer\n",
    "rmsle_train = rmsle_scorer(estimator=regressor_reduced, X=train_features, y_true=train_targets)\n",
    "rmsle_valid = rmsle_scorer(estimator=regressor_reduced, X=valid_features, y_true=valid_targets)\n",
    "rmsle_train, rmsle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomized Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating randomized search grid\n",
    "rscv_grid: Dict[str,list] = {\n",
    "    \"max_depth\": [None, 3, 5, 10],\n",
    "    \"max_features\": [0.5, 1, \"sqrt\"],\n",
    "    \"max_samples\": [10000],\n",
    "    \"min_samples_leaf\": numpy.arange(1, 20, 2),\n",
    "    \"min_samples_split\": numpy.arange(2, 20, 2),\n",
    "    \"n_estimators\": numpy.arange(10, 100, 10)}\n",
    "\n",
    "### creating randomized search cross validation model\n",
    "rscv_model = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=rscv_grid, random_state=42,\n",
    "    n_iter=50, cv=3, scoring=rmsle_scorer, verbose=1)\n",
    "\n",
    "### training model\n",
    "rscv_model.fit(X=train_features, y=train_targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### displaying best parameters\n",
    "rscv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating best model\n",
    "rmsle_train = rmsle_scorer(estimator=rscv_model.best_estimator_, X=train_features, y_true=train_targets)\n",
    "rmsle_valid = rmsle_scorer(estimator=rscv_model.best_estimator_, X=valid_features, y_true=valid_targets)\n",
    "rmsle_train, rmsle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training full model with best hyperparameters**\n",
    "\n",
    "These hyperparameters were found by the course instructor with 100 iterations of randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating random forest regressor with best hyperparameters\n",
    "regressor_best = RandomForestRegressor(\n",
    "    n_estimators=40, min_samples_split=14, min_samples_leaf=1,\n",
    "    max_samples=None, max_features=0.5, max_depth=None,\n",
    "    random_state=42, n_jobs=-1)\n",
    "\n",
    "### training algorithm\n",
    "regressor_best.fit(X=train_features, y=train_targets)\n",
    "\n",
    "### evaluating best model\n",
    "rmsle_train = rmsle_scorer(estimator=regressor_best, X=train_features, y_true=train_targets)\n",
    "rmsle_valid = rmsle_scorer(estimator=regressor_best, X=valid_features, y_true=valid_targets)\n",
    "rmsle_train, rmsle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "The training and test datasets must be in the same format.  \n",
    "The exact same mamipulations must be performed on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing test dataset from file\n",
    "test_df: DataFrame = read_csv(\n",
    "    filepath_or_buffer=\"data-test.csv\",\n",
    "    parse_dates=[\"saledate\"],\n",
    "    low_memory=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### making predictions using optimal regressor\n",
    "test_predicts = optimal_regressor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### formatting predictions as requested by Kaggle and exporting to csv\n",
    "predicts_df = DataFrame(data=[test_df[\"salesID\"], test_predicts], columns=[\"salesID\", \"SalesPrice\"])\n",
    "predicts_df.to_csv(path_or_buf=\"predict-test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
