{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Sale Price of Bulldozers (Kaggle Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict the sale price of bulldozers at auction.  \n",
    "Predictions are based on usage, equipment type, and configuaration.  \n",
    "The data is sourced from auction result postings.  \n",
    "Type of machine learning problem: **supervised learning / time series regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition evaluation metric was *root mean squared log error (RMSLE)*.  \n",
    "**Project goal:** To minimize the difference between actual and predicted prices, i.e., to minimize RMSLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is downloaded from the *Bluebook for Bulldozers* past Kaggle competition:  \n",
    "[Bluebook for Bulldozers Kaggle Competition](https://www.kaggle.com/c/bluebook-for-bulldozers/overview)  \n",
    "There are three main datasets:  \n",
    "* **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012.  \n",
    "The score on this set was used to create the public leaderboard.\n",
    "* **Test.csv** is the test set, which contains data from May 1, 2012 - November 30, 2012.  \n",
    "The score on this set determined the final rank for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data dictionary**\n",
    "\n",
    "Kaggle provided a data dictiorany for these datasets.  \n",
    "See `data-dictionary.xlsx` in the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing standard libraries\n",
    "from typing import List, Dict\n",
    "\n",
    "### importing data analysis libraries\n",
    "import numpy, pandas\n",
    "from pandas import read_csv, DataFrame, Series\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "### importing machine learning libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing dates**\n",
    "\n",
    "Working with time series data requires date/time information to be in python datetime format for easy processing.  \n",
    "Date/time columns are parsed into datetime format using the `parse_dates=` parameter of `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing training and validation datasets from file\n",
    "import_df: DataFrame = read_csv(\n",
    "    filepath_or_buffer=\"../Large-Files/data-train-valid.csv\",\n",
    "    parse_dates=[\"saledate\"],\n",
    "    low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the distribution of target variable\n",
    "bulldozer_sorted[\"SalePrice\"].plot.hist(color=\"steelblue\")\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Distribution of Bulldozer Sale Prices\")\n",
    "pyplot.ylabel(ylabel=\"Sale Count\")\n",
    "pyplot.xlabel(xlabel=\"Sale Price ($)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: Sale date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting correlation between sale price and sale date\n",
    "bulldozer_df[:500].plot.scatter(y=\"SalePrice\", x=\"saledate\", c=\"steelblue\", s=50)\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Sale Price and Sale Date\")\n",
    "pyplot.ylabel(ylabel=\"Sale Price ($)\")\n",
    "pyplot.xlabel(xlabel=\"Sale Date\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring: State of sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the distribution of sales by state\n",
    "bulldozer_sorted[\"state\"].value_counts().sort_index(ascending=True).plot.bar(figsize=(12,5))\n",
    "\n",
    "### customizing the plot\n",
    "pyplot.title(label=\"Distribution of Sales by States\")\n",
    "pyplot.ylabel(ylabel=\"Sale Counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-driven data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "When there are a huge amount of features, it may be better to start building a machine learning model right away.  \n",
    "Model-driven data exploration lets the machine learning algorithm select the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "Feature engineering means processing data in the dataset.  \n",
    "It includes transforming existing data and/or creating new data from existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting dataframe by date**\n",
    "\n",
    "When working with time series, it is better to sort data by date/time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: sorting dataframe by sale date\n",
    "def sortData(df:DataFrame) -> None:\n",
    "    df = df.sort_values(by=\"saledate\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enriching dataset with date information**\n",
    "\n",
    "Using the `pandas.dt` interface, additional data is extracted from datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: extracting date information from sale date column\n",
    "def appendDateInfo(df:DataFrame) -> None:\n",
    "    df[\"saleYear\"] = df[\"saledate\"].dt.year\n",
    "    df[\"saleMonth\"] = df[\"saledate\"].dt.month\n",
    "    df[\"saleDay\"] = df[\"saledate\"].dt.day\n",
    "    df[\"saleDayOfWeek\"] = df[\"saledate\"].dt.day_of_week\n",
    "    df[\"saleDayOfYear\"] = df[\"saledate\"].dt.day_of_year\n",
    "    df = df.drop(columns=\"saledate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing: Cleaning and transforming string values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas api.types interface**\n",
    "\n",
    "Datatypes can be manipulated with the `pandas.api.types` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas category datatype**\n",
    "\n",
    "One way to convert strings into numbers is to use the pandas category datatype.  \n",
    "The category datatype represents missing values with -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas .cat interface**\n",
    "\n",
    "The category datatype is manipulated with the `pandas.cat` interface.  \n",
    "`dataframe[column].cat.categories` = lists all categories in the given dataframe column.  \n",
    "`dataframe[column].cat.codes` = lists all integer codes in the given dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: converting string > category > int and filling missing values\n",
    "def cleanStrings(df:DataFrame) -> None:\n",
    "    for column,values in df.items(): # iterating through dataframe\n",
    "        if pandas.api.types.is_string_dtype(values): # selecting string columns\n",
    "            df[column+\"_missing\"] = values.isna() # saving location of missing values\n",
    "            df[column] = values.astype('category').cat.as_ordered() # converting string > category\n",
    "            values = df[column]\n",
    "            df[column] = values.cat.codes + 1 # converting category > int (missing values = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing: Cleaning numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Filling missing numerical values with any statistical endpoints must be done after splitting the dataset!  \n",
    "Validation / test data in any form must not be used in training the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical concept**\n",
    "\n",
    "The mean is much more sensitive to outliers than the median.  \n",
    "Using the median is advised with large datasets full of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function: filling missing numerical values with median\n",
    "def cleanNumerics(df:DataFrame) -> None:\n",
    "    for column,values in df.items(): # iterating through dataframe\n",
    "        if pandas.api.types.is_numeric_dtype(values): # selecting numeric columns\n",
    "            df[column+\"_missing\"] = values.isna() # saving location of missing values\n",
    "            df[column] = values.fillna(value=values.median()) # filling with median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data: Splitting into training / validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting dataset training / validation\n",
    "training_df: DataFrame = bulldozer_sorted.loc[bulldozer_sorted[\"saleYear\"] != 2012]\n",
    "validation_df: DataFrame = bulldozer_sorted.loc[bulldozer_sorted[\"saleYear\"] == 2012]\n",
    "\n",
    "training_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data: Splitting into features / targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting datasets features / targets\n",
    "features_train: DataFrame = training_df.drop(columns=\"SalePrice\")\n",
    "targets_train: Series = training_df[\"SalePrice\"]\n",
    "features_valid: DataFrame = validation_df.drop(columns=\"SalePrice\")\n",
    "targets_valid: Series = validation_df[\"SalePrice\"]\n",
    "\n",
    "features_train.shape, targets_train.shape, features_valid.shape, targets_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting and training machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepariong data\n",
    "features: DataFrame = bulldozer_sorted.drop(columns=\"SalePrice\")\n",
    "targets: Series = bulldozer_sorted[\"SalePrice\"]\n",
    "\n",
    "### preparing random forest regressor\n",
    "regressor: RandomForestRegressor = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "### training algorithm\n",
    "regressor.fit(X=features, y=targets)\n",
    "\n",
    "### scoring model\n",
    "regressor.score(X=features, y=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom evaluation function\n",
    "def evaluateModel(\n",
    "        model:RandomForestRegressor,\n",
    "        features_train:DataFrame, features_valid:DataFrame,\n",
    "        targets_train:Series, targets_valid:Series):\n",
    "    \"\"\"\n",
    "    Calculates the Root Mean Squared Log Error (RMSLE) for the given model.\n",
    "    \"\"\"\n",
    "    predict_train: Series[float] = model.predict(X=features_train)\n",
    "    predict_valid: Series[float] = model.predict(X=features_valid)\n",
    "    rmsle_train: float = mean_squared_log_error(y_true=targets_train, y_pred=predict_train, squared=False)\n",
    "    rmsle_valid: float = mean_squared_log_error(y_true=targets_valid, y_pred=predict_valid, squared=False)\n",
    "    return rmsle_train, rmsle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning concept**\n",
    "\n",
    "It takes a lot of time to train an algorithm on a large training dataset.  \n",
    "The solution is to use only a fraction of the training dataset for experimenting.  \n",
    "The final model is trained on the entire training dataset using the best hyperparameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`max_samples` argument**\n",
    "\n",
    "With random forest regressor, the fraction of training dataset is specified by using the `max_samples` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training algorithm on a subset of training data\n",
    "regressor = RandomForestRegressor(random_state=42, max_samples=10000, n_jobs=-1)\n",
    "regressor.fit(X=features_train, y=targets_train)\n",
    "scores = evaluateModel(\n",
    "    model=regressor,\n",
    "    features_train=features_train, features_valid=features_valid,\n",
    "    targets_train=targets_train, targets_valid=targets_valid)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating randomized search grid\n",
    "rscv_grid: Dict[str,list] = {\n",
    "    \"max_depth\": [None, 3, 5, 10],\n",
    "    \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n",
    "    \"max_samples\": [10000],\n",
    "    \"min_samples_leaf\": numpy.arange(1, 20, 2),\n",
    "    \"min_samples_split\": numpy.arange(2, 20, 2),\n",
    "    \"n_estimators\": numpy.arange(10, 100, 10)}\n",
    "\n",
    "### creating and fitting randomized search cross validation model\n",
    "rscv_model = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=rscv_grid, n_iter=10, cv=5, verbose=True)\n",
    "rscv_model.fit(X=features_train, y=targets_train)\n",
    "\n",
    "### displaying best parameters\n",
    "rscv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating best model\n",
    "scores = evaluateModel(\n",
    "    model=rscv_model.best_estimator_,\n",
    "    features_train=features_train, features_valid=features_valid,\n",
    "    targets_train=targets_train, targets_valid=targets_valid)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model on full training dataset using best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "These hyperparameters were found by the course instructor with 100 iterations of randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating and training random forest regressor\n",
    "regressor = RandomForestRegressor(\n",
    "    n_estimators=40, min_samples_split=14, min_samples_leaf=1,\n",
    "    max_samples=None, max_features=0.5, random_state=42, n_jobs=-1)\n",
    "regressor.fit(X=features_train, y=targets_train)\n",
    "\n",
    "### evaluating ideal model\n",
    "scores = evaluateModel(\n",
    "    model=regressor,\n",
    "    features_train=features_train, features_valid=features_valid,\n",
    "    targets_train=targets_train, targets_valid=targets_valid)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "\n",
    "The training and test datasets must be in the same format.  \n",
    "The exact same mamipulations must be performed on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing test dataset from file\n",
    "test_df: DataFrame = read_csv(\n",
    "    filepath_or_buffer=\"data-test.csv\",\n",
    "    parse_dates=[\"saledate\"],\n",
    "    low_memory=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### making predictions using optimal regressor\n",
    "test_predicts = optimal_regressor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### formatting predictions as requested by Kaggle and exporting to csv\n",
    "predicts_df = DataFrame(data=[test_df[\"salesID\"], test_predicts], columns=[\"salesID\", \"SalesPrice\"])\n",
    "predicts_df.to_csv(path_or_buf=\"predict-test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
